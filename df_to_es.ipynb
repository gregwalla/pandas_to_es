{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper notebook to load a panda dataframe by chunks into elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/05/beginners-guide-to-data-exploration-using-elastic-search-and-kibana/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting : hello there\n",
      "done in 0.0 s\n",
      "(614, 13)\n",
      "(367, 12)\n"
     ]
    }
   ],
   "source": [
    "####### Libraries & settings\n",
    "import os\n",
    "#import sys\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "#import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from elasticsearch import Elasticsearch #needs to be version 6 max : https://stackoverflow.com/questions/48139986/elastichttperror-406-elastic-search-error-while-indexing-data\n",
    "from elasticsearch.helpers import bulk #https://elasticsearch-py.readthedocs.io/en/master/helpers.html\n",
    "\n",
    "\n",
    "######## functions\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time()\n",
    "    print('\\nstarting :', name)\n",
    "    yield\n",
    "    if time() - t0 < 120:\n",
    "      print('done in', round(time() - t0, 1),  's')\n",
    "    else:\n",
    "      print('done in ', round((time() - t0)/60, 1), 'm')\n",
    " \n",
    "\n",
    "def gendata(index_name, lst):\n",
    "    \"helper function for index_data()\"\n",
    "    for entry in lst:\n",
    "        yield {\n",
    "            \"_index\": index_name,\n",
    "            \"doc\": entry\n",
    "            } \n",
    "\n",
    "def index_data(data_path, chunksize, index_name):\n",
    "    \"sends data to elasticsearch\"\n",
    "    f = open(data_path)\n",
    "    csvfile = pd.read_csv(f, iterator=True, chunksize=chunksize) \n",
    "    es = Elasticsearch() \n",
    "    try :\n",
    "        es.indices.delete(index_name)\n",
    "    except :\n",
    "        pass\n",
    "    es.indices.create(index=index_name, ignore=400)\n",
    "    for i,df in enumerate(csvfile): \n",
    "        records=df.where(pd.notnull(df), None).T.to_dict()\n",
    "        list_records=[records[it] for it in records]\n",
    "        try :\n",
    "            bulk(es, gendata(index_name, list_records))\n",
    "            print(f'sending chunk {i} of {index_name}')\n",
    "            #es.helpers.bulk(index_name, doc_type, list_records)\n",
    "        except :\n",
    "            print(\"error!, skiping chunk!\")\n",
    "            pass    \n",
    "        \n",
    "        \n",
    "######### variables & load\n",
    "now = os.getcwd()\n",
    "subdirs = {'data' : 'data'}\n",
    "\n",
    "files = {'train' : 'train_ctrUa4K.csv',\n",
    "         'test': 'test_lAUu6dG.csv',\n",
    "         'sample_sub' : 'sample_submission_49d68Cx.csv'\n",
    "         }\n",
    "\n",
    "train_data_path = os.path.join(now, subdirs['data'], files['train'])\n",
    "test_data_path = os.path.join(now, subdirs['data'], files['test'])\n",
    "\n",
    "CHUNKSIZE=100\n",
    "index_name_train = 'loan_prediction_train'\n",
    "index_name_test = \"loan_prediction_test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending chunk! 0 of loan_prediction_train\n",
      "sending chunk! 1 of loan_prediction_train\n",
      "sending chunk! 2 of loan_prediction_train\n",
      "sending chunk! 3 of loan_prediction_train\n",
      "sending chunk! 4 of loan_prediction_train\n",
      "sending chunk! 5 of loan_prediction_train\n",
      "sending chunk! 6 of loan_prediction_train\n",
      "sending chunk! 0 of loan_prediction_test\n",
      "sending chunk! 1 of loan_prediction_test\n",
      "sending chunk! 2 of loan_prediction_test\n",
      "sending chunk! 3 of loan_prediction_test\n"
     ]
    }
   ],
   "source": [
    "index_data(train_data_path, 100, index_name_train) # Indexing train data\n",
    "index_data(test_data_path, 100, index_name_test) # Indexing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#end - no need to run\n",
    "\n",
    "#premiminary work\n",
    "with timer('hello there'): \n",
    "    train = pd.read_csv(train_data_path)\n",
    "    test = pd.read_csv(test_data_path)\n",
    "    \n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "#for step by step debug in the function\n",
    "f = open(train_data_path)\n",
    "chunksize = 100\n",
    "csvfile = pd.read_csv(f, iterator=True, chunksize=chunksize) #type : pandas.io.parsers.TextFileReader\n",
    "\n",
    "for i,df in enumerate(csvfile): \n",
    "    records=df.where(pd.notnull(df), None).T.to_dict()\n",
    "    list_records=[records[it] for it in records]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
